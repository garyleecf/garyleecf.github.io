---
title: "Cooperation through Limited Communication"
tags:
  - wireless communications
  - artificial intelligence
  - coding theory
order: 2
header:
  teaser: "assets/images/research/cooperation_and_communication.png"
---

(Based on MIT UROP/ELO offered in Fall 2021)

![cooperation_and_communication]({{ "/assets/images/research/cooperation_and_communication.png" | relative_url }})

<!-- In this project, students will develop learning algorithms for distributed, cooperative multi-agent environments through inductive puzzles and games. In such settings, communication and coordination pose unique challenges, since the optimal course of action of one agent would also depend on the state and actions of others. In particular, we will be studying this problem using the game of Hanabi, and well-known “hat puzzles”. 

The game of Hanabi is a multiplayer, cooperative game that was recently proposed as a new benchmark for artificial intelligence research. In this game, players have imperfect information (can only see the cards of other players, but not their own), and have to develop strategies to effectively exchange information through the limited set of actions allowed. Hence, for an agent to perform well in this game, it would need to go beyond simple observations, but to also reason what the actions of other agents can say about the state of the environment.

Before the development of reinforcement learning algorithms for this problem, the best performing Hanabi-playing agents tended to be handcrafted algorithms based on heuristics. However, such agents tended to perform badly in an ad-hoc setting -- i.e., in a heterogeneous setting where the AI players were developed independently and hence do not share the same strategy/policy. 
Among the handcrafted agents, a notable strategy is the use of coded actions -- the “message” provided by a player conveys additional information beyond its naive interpretation. 

The latter scenario bears much resemblance to the well-known inductive hat puzzles (prisoners having to guess the color of their hats under certain circumstances, with restrictions on their communication/ information exchange). On the one hand, these puzzles tend to have well-established solutions; furthermore, the solutions to some variants of these puzzles have close connections to concepts in coding theory. On the other hand, recent efforts have also attempted to study such problems with the use of reinforcement learning algorithms. However, surprisingly, the optimal strategy that an AI learns may differ from the aforementioned solution. The nonuniqueness in strategies for such puzzles could potentially translate to interesting insights in connection with communication or coding theory.  

The project consists of multiple milestones that the students can seek to achieve. 
First, the students can explore different “hat puzzles” and other inductive puzzles as a sandbox to develop and test different multi-agent reinforcement learning methods.
Second, the students will also consider a variant of the problem -- given that a subset of the agents have a pre-determined strategy/coding scheme, how can new agents with no knowledge about the scheme be integrated into the group and learn to cooperate effectively. In other words, would such an agent be able to learn and infer the strategy only through repeated self-play?
Finally, the students can then synthesize these efforts as they develop algorithms to train AI agents to play Hanabi, and potentially to tackle other more complex distributed multi-agent problems.

As part of this project, students will build a simulation environment to model the various inductive puzzles/”mini-games”, and also design and develop computational solutions to the problem. To aid in their progress, the students will be engaged in weekly discussion sessions with mentors. A short introduction to the relevant concepts and tools would also be provided to the students at the beginning of the project.  -->

